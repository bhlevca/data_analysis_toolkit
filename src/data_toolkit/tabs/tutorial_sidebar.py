"""
Tutorial Sidebar module for the Data Analysis Toolkit
"""

import streamlit as st

# Import accelerated functions
try:
    from rust_accelerated import AccelerationSettings, is_rust_available
except ImportError:
    from .rust_accelerated import AccelerationSettings, is_rust_available


TUTORIALS = {
    "getting_started": """
## üöÄ Getting Started with the Advanced Data Analysis Toolkit

Welcome! This toolkit provides comprehensive data analysis capabilities organized into **7 main tabs** with **subtabs** inside each group.

---

### üìÇ Tab Structure (7 Main Tabs ‚Üí Subtabs)

**Click a main tab, then click a subtab inside to access specific features:**

| Main Tab | Subtabs Inside |
|----------|----------------|
| **üìÅ Data** | Data Loading (upload files, select columns) |
| **üìä Statistics** | Descriptive Statistics, Hypothesis Tests, Bayesian Inference, Uncertainty Analysis |
| **üîä Signal Processing** | FFT/Wavelet (frequency and time-frequency analysis) |
| **‚è±Ô∏è Time Series** | Time Series Analysis, Causality (Granger) |
| **ü§ñ Machine Learning** | Regression/Classification, üß† Neural Networks, PCA, Clustering, Anomaly Detection, Dimensionality Reduction, Non-Linear Analysis |
| **üìà Visualization** | Interactive Plots (scatter, box, 3D, regression lines) |
| **üñºÔ∏è Image** | Image Recognition (training, prediction, dataset formats) |

---

### Step-by-Step Workflow

**Step 1: Load Your Data**
1. Click **üìÅ Data** main tab
2. Click **"Browse files"** and select a CSV or Excel file
3. Your data preview appears automatically

**Step 2: Select Columns**
- **Feature Columns**: Independent variables (X) - what you use to predict
- **Target Column**: Dependent variable (Y) - what you want to predict

**Step 3: Navigate to Your Analysis**

| Your Question | Main Tab ‚Üí Subtab |
|---------------|-------------------|
| Basic statistics? | üìä Statistics ‚Üí Descriptive Statistics |
| Significant difference? | üìä Statistics ‚Üí Hypothesis Tests |
| Confidence intervals? | üìä Statistics ‚Üí Uncertainty Analysis |
| Frequency content? | üîä Signal Processing ‚Üí FFT/Wavelet |
| Time patterns? | ‚è±Ô∏è Time Series ‚Üí Analysis |
| Does X cause Y? | ‚è±Ô∏è Time Series ‚Üí Causality |
| Predict values? | ü§ñ ML ‚Üí Regression/Classification |
| Deep learning? | ü§ñ ML ‚Üí üß† Neural Networks |
| Reduce dimensions? | ü§ñ ML ‚Üí PCA or Dimensionality Reduction |
| Find clusters? | ü§ñ ML ‚Üí Clustering |
| Find outliers? | ü§ñ ML ‚Üí Anomaly Detection |
| Visualize data? | üìà Visualization ‚Üí Plots |

---

### üìà Interactive Charts (Plotly)

All charts are **fully interactive**:
| Action | How |
|--------|-----|
| **Zoom** | Click and drag |
| **Pan** | Hold Shift and drag |
| **Hover** | See exact values |
| **Download** | Click camera icon |
| **Reset** | Double-click |
""",

    "data_loading": """
## üìÅ Data Loading Tab Guide

**Location:** Main Tab: üìÅ Data

This is your starting point for any analysis.

### How to Load Data

1. **Click "Browse files"** button
2. **Select your file** (CSV or Excel supported)
3. **Preview appears** automatically showing first rows

### Supported File Formats
| Format | Extensions | Notes |
|--------|------------|-------|
| **CSV** | .csv | Comma-separated values |
| **Excel** | .xlsx, .xls | Microsoft Excel files |

### Selecting Columns

After loading data, you need to select:

**Feature Columns (X variables):**
- Independent variables / predictors
- Used as input for models
- Select multiple for multivariate analysis

**Target Column (Y variable):**
- Dependent variable / response
- What you want to predict or explain
- Select ONE column

### Column Selection Tips

| Analysis Type | Features | Target |
|---------------|----------|--------|
| Regression | Predictor variables | Numeric outcome |
| Classification | Predictor variables | Categorical label |
| Clustering | All variables to cluster | Not needed |
| PCA | All numeric variables | Not needed |
| Correlation | All variables of interest | Not needed |

### Data Quality Checks

The app automatically shows:
- **Row count**: Number of observations
- **Column count**: Number of variables
- **Data types**: Numeric, categorical, datetime
- **Missing values**: Highlighted if present

üí° **Tip**: Always check your data preview before analysis!
""",

    "statistical": """
## üìä Descriptive Statistics Subtab Guide

**Location:** Main Tab: üìä Statistics ‚Ä∫ Subtab: Descriptive Statistics

### Buttons Available:
1. **üìà Descriptive Statistics** - Summary stats for selected columns
2. **üîó Correlation Matrix** - Relationship heatmap
3. **üéØ Outlier Detection** - Find unusual values

---

### üìà Descriptive Statistics
Shows: Mean, Median, Std, Min, Max, Skewness, Kurtosis

**Use when:** First loading data to understand basic properties.

---

### üîó Correlation Matrix
**Methods:**
| Method | When to Use |
|--------|-------------|
| **Pearson** | Linear relationships (default) |
| **Spearman** | Non-normal data, ranks |
| **Kendall** | Small samples, ordinal |

**Reading values:**
- **|r| > 0.7**: Strong
- **|r| 0.3-0.7**: Moderate
- **|r| < 0.3**: Weak

---

### üéØ Outlier Detection
| Method | Best For |
|--------|----------|
| **IQR** | Robust, any distribution |
| **Z-score** | Normal distributions |

| Method | How It Works | Best For |
|--------|--------------|----------|
| **IQR (Interquartile Range)** | Values outside Q1 - 1.5√óIQR or Q3 + 1.5√óIQR | Robust, any distribution |
| **Z-score** | Values more than 3 standard deviations from mean | Normal distributions |

üí° **Tip**: Always check for outliers before running machine learning models!
""",

    "machine_learning": """
## ü§ñ ML Models Tab Guide

**Location:** 9th tab (ü§ñ ML Models) in the Machine Learning group

### Buttons Available:
1. **üéØ Train Model** - Train on current data
2. **üîÑ Cross-Validation** - Test model robustness
3. **üìä Feature Importance** - See which features matter
4. **üîÆ Predict** - Apply model to new data

---

### Workflow

**Step 1: Train Model**
- Select Task Type: Regression or Classification
- Choose a model from dropdown
- Click **üéØ Train Model**
- **Plot shows:** Training data actual values (blue dots)

**Step 2: Predict on New Data**
- Upload a new CSV with same feature columns
- Click **üîÆ Predict**
- **Plot shows:**
  - üîµ Blue = Training data (actual values)
  - üî¥ Red = Predictions for new data

---

### Regression Models

| Model | Description |
|-------|-------------|
| **Linear Regression** | y = mx + b baseline |
| **Ridge** | L2 regularization |
| **Lasso** | L1 regularization (feature selection) |
| **ElasticNet** | L1 + L2 combined |
| **Decision Tree** | Tree-based splits |
| **KNN Regressor** | K-Nearest Neighbors |
| **SVR** | Support Vector Regression |
| **Random Forest** | Ensemble of trees |
| **Gradient Boosting** | Sequential boosting |

### Classification Models

| Model | Description |
|-------|-------------|
| **Logistic Regression** | Probabilistic classifier |
| **SVM** | Support Vector Machine |
| **Decision Tree** | Rule-based |
| **KNN** | Majority vote |
| **Naive Bayes** | Probabilistic |
| **Random Forest** | Ensemble |
| **Gradient Boosting** | Max accuracy |

### Metrics
**Regression:** R¬≤, RMSE, MAE, MSE
**Classification:** Accuracy, Precision, Recall, F1

üí° **Tip**: Train first, then upload new data for prediction!
""",

    "pca": """
## üî¨ PCA (Principal Component Analysis) Guide

### What is PCA?
PCA is a **dimensionality reduction** technique that transforms your data into a new coordinate system where:
- **PC1 (Principal Component 1)** captures the most variance
- **PC2** captures the second-most variance
- And so on...

### When to Use PCA
- **Too many features**: Reduce from 100s of columns to a few components
- **Visualization**: Plot high-dimensional data in 2D or 3D
- **Multicollinearity**: Remove correlated features before regression
- **Feature extraction**: Create composite features
- **Noise reduction**: Keep only high-variance components

### Interpreting PCA Results

**Explained Variance:**
- Shows how much information each component captures
- **Cumulative variance**: Total explained by first N components
- **95% rule**: Often keep enough components to explain 95% of variance

**Scree Plot:**
- Bar chart of variance per component
- Look for the **"elbow"** - where gains become small
- Components after the elbow add little information

**Biplot (PCA with Vectors):**
- Points = data samples projected onto PC1/PC2
- Arrows = original feature loadings (direction and strength)
- Arrow length = feature importance
- Arrow direction = correlation with components

### Mathematical Background
PCA finds eigenvectors of the covariance matrix:
- Eigenvectors ‚Üí Principal Component directions
- Eigenvalues ‚Üí Variance explained by each component

üí° **Tip**: Always standardize data before PCA (handled automatically)!
""",

    "bayesian": """
## üìà Bayesian Analysis Guide

### What is Bayesian Analysis?
Unlike classical statistics that gives single-point estimates, Bayesian analysis provides **probability distributions** for parameters.

**Key Concept:** `Posterior ‚àù Likelihood √ó Prior`

### Methods Available

**üìä Bayesian Regression**
- Estimates regression coefficients as distributions
- Shows uncertainty in each coefficient
- Wider distribution = more uncertainty

**üìê Credible Intervals (CI)**
- Bayesian version of confidence intervals
- 95% CI: "There's 95% probability the true value is in this range"
- **Note**: Unlike frequentist CIs, this is a probability statement!

**üìè Posterior Distributions**
- Full probability distribution for each parameter
- Can answer: "What's the probability coefficient > 0?"
- More informative than single-point estimates

**üîÑ Model Comparison (BIC)**
- **BIC (Bayesian Information Criterion)**: Lower = better model
- Penalizes complexity to prevent overfitting
- Compare multiple models: choose lowest BIC

### Interpreting Results
- **Narrow posteriors**: High certainty about parameter value
- **Wide posteriors**: High uncertainty (need more data)
- **Posterior mean**: Best estimate of parameter

üí° **Tip**: Wide credible intervals suggest you need more data!
""",

    "uncertainty": """
## üé≤ Uncertainty Analysis Guide

Understanding uncertainty is crucial for making reliable predictions.

### Bootstrap Confidence Intervals
**What it does:** Resamples your data 1000+ times to estimate confidence intervals.

**When to use:**
- Data may not be normally distributed
- Want robust confidence intervals
- Small sample sizes

**Interpretation:**
- 95% CI: True value falls in this range 95% of the time
- Narrow CI = high confidence
- Wide CI = high uncertainty

---

### Residual Analysis
**Always do this after fitting any regression model!**

**Residual = Actual - Predicted**

**What to check:**
| Test | Good Result | Problem If Not |
|------|-------------|----------------|
| **Q-Q Plot** | Points on diagonal | Non-normal errors |
| **Residuals vs Fitted** | Random scatter | Heteroscedasticity |
| **Durbin-Watson ‚âà 2** | No autocorrelation | Patterns in time |

**Patterns to watch for:**
- **Funnel shape**: Variance changes with fitted values
- **Curve**: Non-linear relationship missed
- **Clusters**: Missing categorical variable

---

### Monte Carlo Simulation
**What it does:** Runs thousands of random simulations to propagate uncertainty.

**Use for:**
- Uncertainty propagation through complex models
- Risk assessment
- Sensitivity analysis

---

### Prediction Intervals
**Difference from Confidence Intervals:**
- **CI**: Uncertainty about mean prediction
- **PI**: Uncertainty about individual prediction (wider!)

üí° **Tip**: Always report prediction intervals, not just point predictions!
""",

    "nonlinear": """
## üîÄ Non-Linear Analysis Guide

Standard Pearson correlation only detects **linear** relationships. These methods detect **any** relationship!

### Distance Correlation
**What it measures:** Any type of dependence (linear, quadratic, periodic, etc.)

| Relationship | Pearson r | Distance Correlation |
|-------------|-----------|---------------------|
| y = x | High (~1) | High (~1) |
| y = x¬≤ | ~0 | High (>0.5) |
| y = sin(x) | ~0 | High (>0.5) |
| y = |x| | ~0 | High (>0.5) |
| Independent | ~0 | ~0 |

**Interpretation:**
- **0**: Independent (no relationship)
- **1**: Perfect dependence
- Values 0-1: Strength of any dependence

---

### Mutual Information (MI)
**Information-theoretic measure of dependence.**

Based on entropy: How much does knowing X reduce uncertainty about Y?

**Interpretation:**
- **MI = 0**: Independent (knowing X tells nothing about Y)
- **Higher MI**: More information shared
- No upper bound (depends on entropy of variables)

---

### Polynomial Regression
**Fits curves instead of straight lines:**
- Degree 2: y = ax¬≤ + bx + c (parabola)
- Degree 3: y = ax¬≥ + bx¬≤ + cx + d (cubic)

**Caution:** High degrees can overfit!

---

### Gaussian Process Regression (GPR)
**Non-parametric Bayesian regression.**
- No fixed functional form
- Provides uncertainty estimates
- Good for small datasets with complex patterns

üí° **Tip**: If Pearson is low but Distance Correlation is high, you have a non-linear relationship!
""",

    "timeseries": """
## ‚è±Ô∏è Time Series Analysis Guide

Time series data has observations ordered in time. Special methods account for temporal dependencies.

### Step 1: Check Stationarity (ADF Test)
**Stationarity**: Statistical properties don't change over time.

| ADF p-value | Interpretation | Action |
|-------------|----------------|--------|
| p < 0.05 | Stationary ‚úÖ | Proceed with analysis |
| p ‚â• 0.05 | Non-stationary ‚ùå | Difference the data |

**Why it matters:** Most time series methods assume stationarity!

---

### Step 2: ACF and PACF Plots
**ACF (Autocorrelation Function):** Correlation with lagged values.
**PACF (Partial ACF):** Correlation after removing intermediate lags.

**Pattern Recognition:**
| ACF | PACF | Model |
|-----|------|-------|
| Tails off | Cuts off at lag p | AR(p) - Autoregressive |
| Cuts off at lag q | Tails off | MA(q) - Moving Average |
| Tails off | Tails off | ARMA(p,q) |

---

### Step 3: Seasonal Decomposition
**Breaks time series into components:**
- **Trend**: Long-term direction
- **Seasonal**: Repeating patterns
- **Residual**: Random noise

**Model types:**
- **Additive**: Y = Trend + Seasonal + Residual
- **Multiplicative**: Y = Trend √ó Seasonal √ó Residual

---

### Step 4: ARIMA Modeling
**ARIMA(p, d, q):**
- **p**: Autoregressive order (past values)
- **d**: Differencing order (0 if stationary)
- **q**: Moving average order (past errors)

**Common choices:**
- ARIMA(1,1,1): Simple baseline
- ARIMA(1,1,0): AR(1) with one difference
- ARIMA(0,1,1): MA(1) with one difference

üí° **Tip**: Always check stationarity first!
""",

    "causality": """
## üîó Causality Analysis Guide

### ‚ö†Ô∏è Critical Warning: Correlation ‚â† Causation!
These methods test **predictive** causality, not true causation.

### Granger Causality Test
**Question:** Does knowing X improve predictions of Y?

**Interpretation:**
| p-value | Result |
|---------|--------|
| p < 0.05 | X "Granger-causes" Y (X helps predict Y) |
| p ‚â• 0.05 | X does NOT Granger-cause Y |

**Important notes:**
- "Granger causality" ‚â† true causation
- Based on temporal precedence only
- Both variables might be caused by a third factor

---

### Lead-Lag Analysis
**Finds optimal time shift between variables.**

**Interpretation:**
| Lag | Meaning |
|-----|---------|
| Lag < 0 | Feature LEADS target (happens before) |
| Lag = 0 | Simultaneous (no lead/lag) |
| Lag > 0 | Target LEADS feature (happens before) |

**Use cases:**
- Economic indicators leading GDP
- Weather leading crop yields
- Advertising leading sales

---

### Bidirectional Causality
Tests both directions:
- Does X Granger-cause Y?
- Does Y Granger-cause X?

**Possible outcomes:**
- X ‚Üí Y only (unidirectional)
- Y ‚Üí X only (unidirectional)
- X ‚Üî Y (bidirectional)
- No causality in either direction

üí° **Tip**: Granger causality only works with stationary time series!
""",

    "visualization": """
## üìà Visualization Guide

All charts use **Plotly** for full interactivity.

### Interactive Controls
| Action | How |
|--------|-----|
| **Zoom** | Click and drag |
| **Pan** | Shift + drag |
| **Hover** | Mouse over for values |
| **Download** | Camera icon ‚Üí PNG |
| **Reset** | Double-click |

### Available Plot Types

| Plot | Best For |
|------|----------|
| **Scatter Matrix** | Overview of all pairwise relationships |
| **Correlation Heatmap** | Visualizing correlation matrix |
| **Box Plots** | Distribution and outliers |
| **Distribution Plots** | Histograms with density |
| **3D Scatter** | Three-variable relationships |
| **Parallel Coordinates** | High-dimensional data |
| **Linear Regression Plot** | Scatter with regression line + statistics |

### Linear Regression Plot
Shows scatter plot with:
- **Best-fit line**: y = slope √ó x + intercept
- **Slope**: Change in Y per unit change in X
- **Intercept**: Y value when X = 0
- **R¬≤ (R-squared)**: Variance explained (0-1)
- **p-value**: Statistical significance of slope

**Interpreting R¬≤:**
| R¬≤ Value | Interpretation |
|----------|----------------|
| > 0.9 | Excellent fit |
| 0.7 - 0.9 | Good fit |
| 0.5 - 0.7 | Moderate fit |
| < 0.5 | Poor fit |

üí° **Tip**: Use scatter matrix first to explore, then zoom in on interesting pairs!
""",

    "anomaly": """
## üö® Anomaly Detection Guide

Anomaly detection identifies **unusual data points** that don't fit the normal pattern. This toolkit provides several robust methods, each with unique strengths and parameters. Below is a detailed guide to each method, their arguments, and what to expect from their results.

### Methods Available

| Method | Algorithm | Best For |
|--------|-----------|----------|
| **Isolation Forest** | Random tree isolation | General purpose, scalable, works well for most data |
| **LOF (Local Outlier Factor)** | Local density comparison | Detecting local anomalies, clusters, density-based outliers |
| **MCD (Minimum Covariance Determinant)** | Robust covariance | Elliptical distributions, robust to outliers |
| **One-Class SVM** | Support Vector Machine | High-dimensional data, flexible kernel choices |
| **DBSCAN** | Density-based clustering | Arbitrary shapes, noise, spatial/irregular clusters |
| **Autoencoder** | Neural network reconstruction | Complex, non-linear patterns, deep learning, high-dimensional data |

---

### Method Details & Arguments

#### **Isolation Forest**
- **How it works:** Randomly isolates points; anomalies are isolated faster.
- **Key argument:** `contamination` (float, 0-1): Expected fraction of anomalies (e.g., 0.05 = 5%).
- **What to expect:** Fast, general-purpose. Good for large datasets. Returns anomaly scores and binary labels.

#### **LOF (Local Outlier Factor)**
- **How it works:** Compares local density of a point to its neighbors.
- **Key arguments:**
    - `n_neighbors` (int): Number of neighbors to use (default: 20).
    - `contamination` (float): Expected fraction of anomalies.
- **What to expect:** Detects local anomalies, can find outliers in clusters. Sensitive to `n_neighbors`.

#### **MCD (Minimum Covariance Determinant)**
- **How it works:** Fits an ellipse to the data, robustly estimates covariance.
- **Key argument:** `contamination` (float): Expected fraction of anomalies.
- **What to expect:** Best for data with elliptical/normal structure. Robust to outliers, but less effective for non-elliptical data.

#### **One-Class SVM**
- **How it works:** Learns a boundary around the majority of data using support vectors.
- **Key arguments:**
    - `nu` (float, 0-1): Upper bound on the fraction of anomalies (default: 0.05).
    - `kernel` (str): Kernel type (`'rbf'`, `'linear'`, etc.; default: `'rbf'`).
    - `gamma` (str or float): Kernel coefficient (`'scale'` or `'auto'` or a float).
- **What to expect:** Flexible, works for high-dimensional data. Can be slow for large datasets. Sensitive to kernel and nu.

#### **DBSCAN**
- **How it works:** Groups points into dense clusters; points not in any cluster are labeled as anomalies.
- **Key arguments:**
    - `eps` (float): Maximum distance between neighbors (default: 0.5).
    - `min_samples` (int): Minimum points to form a cluster (default: 5).
- **What to expect:** Finds arbitrarily shaped clusters, labels noise as anomalies. No need to specify number of clusters. Good for spatial data.

#### **Autoencoder**
- **How it works:** Neural network learns to reconstruct input; anomalies have high reconstruction error.
- **Key arguments:**
    - `encoding_dim` (int): Size of bottleneck layer (default: 8).
    - `contamination` (float): Expected anomaly rate (default: 0.05).
    - `hidden_layers` (list): Encoder architecture (e.g., [64, 32]).
- **What to expect:** Powerful for complex, non-linear data. Needs more data (500+ samples recommended). Returns reconstruction errors, threshold, and anomaly indices.

---

### Output Interpretation
- **Normal points**: Label `1` (inliers)
- **Anomalies**: Label `-1` (outliers)
- **Anomaly scores**: Higher = more likely to be an anomaly (except for DBSCAN, where noise is labeled `-1`)
- **Scatter plot**: Anomalies shown in **red**

### Use Cases
- Fraud detection
- System monitoring
- Quality control
- Data cleaning
- Scientific discovery

---

### Choosing a Method
- **Start with Isolation Forest** for general use.
- **Try LOF** for local anomalies or clusters.
- **Use MCD** for elliptical/normal data.
- **One-Class SVM** for high-dimensional or kernel-based separation.
- **DBSCAN** for spatial/irregular clusters or when you expect noise.
- **Autoencoder** for complex, high-dimensional, or non-linear data (requires more data).

---

üí° **Tip**: Adjust the main parameter (contamination, nu, eps, etc.) to tune sensitivity. Always visualize results and check if detected anomalies make sense for your domain!
""",

    "clustering": """
## üéØ Clustering Guide

Clustering groups similar data points together **without predefined labels**.

### Methods Available

| Method | Description | Clusters Must Be... |
|--------|-------------|---------------------|
| **K-Means** | Minimizes within-cluster variance | Spherical, similar size |
| **Hierarchical (Agglomerative)** | Builds tree of clusters | Any shape |
| **DBSCAN** | Density-based clustering | Arbitrary shape, handles noise |
| **Gaussian Mixture (GMM)** | Probabilistic soft clustering | Elliptical |

### Parameters

**K-Means & GMM:**
- **n_clusters**: Number of clusters (must specify)

**DBSCAN:**
- **eps**: Maximum distance between neighbors
- **min_samples**: Minimum points to form cluster

### Quality Metrics

| Metric | Range | Better |
|--------|-------|--------|
| **Silhouette Score** | -1 to 1 | Higher (>0.5 good) |
| **Davies-Bouldin Index** | 0 to ‚àû | Lower |
| **Calinski-Harabasz** | 0 to ‚àû | Higher |

### Choosing Number of Clusters
1. **Elbow method**: Plot within-cluster variance vs k
2. **Silhouette analysis**: Maximize silhouette score
3. **Domain knowledge**: What makes sense for your data?

üí° **Tip**: K-Means is fast but assumes spherical clusters. Try DBSCAN if shapes are irregular!
""",

    "dim_reduction": """
## üìâ Dimensionality Reduction Guide

Reduces high-dimensional data to fewer dimensions for visualization or preprocessing.

### Methods Available

| Method | Type | Preserves |
|--------|------|-----------|
| **PCA (Principal Component Analysis)** | Linear | Global variance |
| **SVD (Singular Value Decomposition)** | Linear | Works on sparse matrices |
| **t-SNE (t-distributed Stochastic Neighbor Embedding)** | Non-linear | Local structure |
| **UMAP (Uniform Manifold Approximation)** | Non-linear | Local + some global |
| **ICA (Independent Component Analysis)** | Linear | Statistical independence |

### When to Use Each

| Scenario | Best Method |
|----------|-------------|
| Feature extraction for ML | PCA |
| Visualization of clusters | t-SNE or UMAP |
| Sparse data (text) | SVD (TruncatedSVD) |
| Signal separation | ICA |
| Preserving distances | UMAP |

### Parameters

**t-SNE:**
- **Perplexity**: Balance local/global (5-50, default 30)
- Higher = more global structure

**UMAP:**
- **n_neighbors**: Local neighborhood size (5-50)
- **min_dist**: How tight clusters are (0.0-1.0)

### Interpretation
- **2D/3D plots**: Look for clusters, patterns
- **Not for prediction**: Just for visualization/preprocessing

üí° **Tip**: PCA is deterministic; t-SNE/UMAP may give different results each run!
""",

    "signal_analysis": """
## üîä Signal Analysis Guide

Analyze frequency content and time-frequency patterns in signals.

### FFT (Fast Fourier Transform)
**Transforms time domain ‚Üí frequency domain**

**Output:**
- **Frequencies (Hz)**: X-axis
- **Magnitude**: Amplitude at each frequency
- **Dominant frequency**: Strongest component

**Interpretation:**
- **Peak** at frequency f ‚Üí signal has component oscillating at f Hz
- **Multiple peaks** ‚Üí multiple frequency components
- **Nyquist limit**: Can only detect up to (sampling_rate / 2) Hz

---

### PSD (Power Spectral Density)
**Power distribution across frequencies**

Uses Welch's method (windowed FFT) for:
- Noise reduction
- Smoother spectrum
- More reliable peak detection

---

### CWT (Continuous Wavelet Transform)
**Time-frequency representation**

Shows how frequency content **changes over time**.

**Output:**
- **X-axis**: Time
- **Y-axis**: Frequency (or scale)
- **Color**: Power/magnitude

**Features:**
- **COI (Cone of Influence)**: Edge effects region
- **Significance contours**: Statistically significant features

**Wavelet types:**
- **Morlet (morl)**: Good frequency resolution
- **Mexican hat (mexh)**: Good time resolution
- **Gaussian (gaus)**: General purpose

---

### DWT (Discrete Wavelet Transform)
**Multi-scale decomposition**

Decomposes signal into:
- **Approximation coefficients**: Low-frequency trend
- **Detail coefficients**: High-frequency details at each level

**Wavelet types:**
- **db4, db8**: Daubechies (good all-around)
- **haar**: Simplest, good for step detection
- **sym4**: Symmetric Daubechies
- **coif1**: Coiflet (good for smooth signals)

**Use cases:**
- Denoising
- Feature extraction
- Compression

üí° **Tip**: CWT for visualization, DWT for numerical analysis!
""",

    "tests": """
## üß™ Statistical Hypothesis Tests Guide

Formal tests for statistical significance.

### Comparing Two Groups

| Test | Assumption | Null Hypothesis |
|------|------------|-----------------|
| **Independent t-test** | Normal, equal variance | Œº‚ÇÅ = Œº‚ÇÇ (means equal) |
| **Welch's t-test** | Normal, unequal variance | Œº‚ÇÅ = Œº‚ÇÇ |
| **Mann-Whitney U** | Non-parametric | Distributions equal |
| **Paired t-test** | Normal, paired samples | Mean difference = 0 |
| **Wilcoxon Signed-Rank** | Non-parametric, paired | Distributions equal |

### Comparing 3+ Groups

| Test | Assumption | Null Hypothesis |
|------|------------|-----------------|
| **One-way ANOVA** | Normal, equal variance | All means equal |
| **Kruskal-Wallis** | Non-parametric | All distributions equal |

### Chi-Square Tests

| Test | Use For |
|------|---------|
| **Chi-Square Independence** | Are two categorical variables related? |
| **Chi-Square Goodness-of-Fit** | Does data fit expected distribution? |

### Normality Tests

| Test | Description |
|------|-------------|
| **Shapiro-Wilk** | Best for small samples (<50) |
| **Kolmogorov-Smirnov** | Works for any sample size |
| **Anderson-Darling** | More sensitive to tails |

### Correlation Tests

Tests if correlation coefficient is significantly different from 0.

### Reading p-values

| p-value | Interpretation |
|---------|----------------|
| p < 0.001 | Very strong evidence against null |
| p < 0.01 | Strong evidence |
| p < 0.05 | Moderate evidence (common threshold) |
| p ‚â• 0.05 | Insufficient evidence to reject null |

### Effect Size
p-value tells significance, not importance. Also report:
- **Cohen's d**: Standardized mean difference
- **R¬≤**: Variance explained
- **Correlation coefficient**: Strength of relationship

üí° **Tip**: Statistical significance ‚â† practical significance. Always consider effect size!
""",

    "neural_networks": """
## üß† Neural Networks Guide

Deep learning models for regression, forecasting, and anomaly detection.

### Available Models

| Model | Use Case | Best For |
|-------|----------|----------|
| **MLP Regressor** | Predict continuous values | Non-linear regression |
| **MLP Classifier** | Classify categories | Multi-class classification |
| **LSTM Forecast** | Time series prediction | Sequential patterns |
| **Autoencoder** | Anomaly detection | Finding outliers |

---

### üîÆ MLP (Multi-Layer Perceptron)

Feedforward neural network with customizable architecture.

**Parameters:**
| Parameter | Default | Description |
|-----------|---------|-------------|
| `hidden_layers` | [64, 32] | Neurons per layer |
| `activation` | relu | Activation function |
| `dropout_rate` | 0.2 | Regularization (0-0.5) |
| `epochs` | 100 | Training iterations |
| `batch_size` | 32 | Samples per update |

**Output Metrics (Regression):**
- **RMSE**: Root Mean Square Error (lower is better)
- **MAE**: Mean Absolute Error
- **R¬≤**: Coefficient of determination (closer to 1 is better)

**Tips:**
- Use 100+ epochs for complex patterns
- Reduce dropout for small datasets
- Add more layers for highly non-linear relationships

---

### üìà LSTM (Long Short-Term Memory)

Specialized for time series with temporal dependencies.

**Parameters:**
| Parameter | Default | Description |
|-----------|---------|-------------|
| `sequence_length` | 20 | Lookback window (past steps to use) |
| `forecast_horizon` | 10 | Future steps to predict |
| `lstm_units` | 64 | Neurons per LSTM layer |
| `n_lstm_layers` | 2 | Number of LSTM layers |

**When to Use:**
- Data has temporal patterns
- Sequential dependencies matter
- Need to forecast future values

**Tips:**
- sequence_length should capture one full cycle/pattern
- More data = better forecasts (100+ samples minimum)
- Use for univariate time series (single column)

---

### üö® Autoencoder (Anomaly Detection)

Learns to compress and reconstruct data. Anomalies have high reconstruction error.

**Parameters:**
| Parameter | Default | Description |
|-----------|---------|-------------|
| `encoding_dim` | 8 | Bottleneck size (compression) |
| `contamination` | 0.05 | Expected anomaly rate (5%) |
| `hidden_layers` | [64, 32] | Encoder architecture |

**How It Works:**
1. **Encoder**: Compresses data to low dimensions
2. **Decoder**: Reconstructs original data
3. **Error**: Normal data reconstructs well, anomalies don't
4. **Threshold**: Set by contamination rate

**Output:**
- `reconstruction_errors`: Error per sample
- `threshold`: Cutoff for anomaly classification
- `anomaly_indices`: Which samples are anomalies

---

### ‚ö†Ô∏è Important Notes

**Data Requirements:**
- Neural networks need MORE data than traditional ML
- Minimum: 500+ samples (1000+ recommended)
- More features = need more data

**When NOT to Use:**
- Small datasets (< 200 samples) ‚Üí Use Random Forest instead
- Simple linear relationships ‚Üí Use Linear Regression
- Need interpretability ‚Üí Use Decision Trees

**Training Tips:**
- Watch for overfitting (val_loss increasing while loss decreases)
- Start with fewer epochs, increase if needed
- Use validation split to monitor generalization

---

### üîß Troubleshooting

| Problem | Solution |
|---------|----------|
| Predictions always same value | Increase epochs, reduce dropout |
| Overfitting (val_loss >> loss) | Increase dropout, reduce layers |
| Training too slow | Reduce batch_size, fewer layers |
| Poor accuracy | More data, more epochs, tune architecture |

üí° **Tip**: Start simple (2 layers, 64 neurons) and add complexity only if needed!
""",

    "image_recognition": """
## üñºÔ∏è Image Recognition (Streamlit Image Tab)

Step-by-step:
1) **Select data folder** ‚Äî provide class subfolders or upload a `labels.csv` mapping filenames to labels (prefer relative paths).
2) **Preview & validate** ‚Äî inspect a few images per class; check for missing or corrupt files.
3) **Configure** ‚Äî set `image_size` (64‚Äì224), `batch_size` (16‚Äì64), `epochs`, and enable augmentation if needed.
4) **Choose model** ‚Äî CNN from scratch or Transfer Learning (recommended for small datasets).
5) **Train** ‚Äî use early stopping on validation loss; monitor accuracy and per-class metrics.
6) **Save** ‚Äî enable "Save model automatically" or use the Save trained model button; prefer `.keras` format. If class names are missing, predictions will show numeric labels.

Troubleshooting: Verify `labels.csv` paths, re-save in `.keras` to avoid HDF5 issues, and register custom layers used by the model.
""",

    "anova_extended": """
## üß™ Extended ANOVA & Post-Hoc Tests

### Two-Way ANOVA
Tests main effects of two factors and their interaction on a continuous outcome.
- **Main Effect A**: Does factor A affect the outcome?
- **Main Effect B**: Does factor B affect the outcome?
- **Interaction A√óB**: Does the effect of A depend on B?

### Repeated-Measures ANOVA
Use when the same subjects are measured under multiple conditions.
- Accounts for within-subject correlation
- More powerful than independent-groups ANOVA
- Check sphericity assumption (Mauchly's test)

### Post-Hoc Tests
After significant ANOVA, determine which groups differ:

| Test | Use When |
|------|----------|
| **Tukey's HSD** | All pairwise comparisons, balanced groups |
| **Bonferroni** | Conservative, few comparisons |

üí° **Tip**: Run post-hoc tests only after significant omnibus ANOVA!
""",

    "probability_distributions": """
## üìä Probability Distribution Fitting

### Extended Distribution Fitting
Fits 12+ theoretical distributions to your data:
- Normal, t-distribution, Gamma, Exponential
- Lognormal, Weibull, Laplace, Logistic
- Pareto, Cauchy, and more

### Model Selection Criteria
| Metric | Interpretation |
|--------|----------------|
| **AIC** | Lower is better (penalizes complexity) |
| **BIC** | Lower is better (stricter penalty) |
| **KS test** | p > 0.05 means good fit |

### Random Variable Analysis
- Moments: mean, variance, skewness, kurtosis
- Quantiles: median, quartiles, percentiles
- Confidence intervals for parameters

### QQ Plots
Points on diagonal = good fit to theoretical distribution.

üí° **Tip**: Compare multiple distributions using AIC before choosing one!
""",

    "spectral_extended": """
## üîä Extended Spectral Analysis

### Coherence Analysis
Measures linear correlation between two signals as a function of frequency.
- **Value 0-1**: Higher = stronger relationship at that frequency
- Use for finding common oscillations in paired signals

### Cross-Wavelet Transform (XWT)
Shows where two signals share common power in time-frequency space.
- Reveals coupled oscillations
- Identifies phase relationships

### Wavelet Coherence (WTC)
Localized coherence varying in both time and frequency.
- Better than standard coherence for non-stationary signals
- Shows when/where signals are correlated

### Harmonic Analysis
Least-squares fitting of sinusoidal components.
- Extracts dominant periodicities
- Provides amplitude, frequency, and phase

üí° **Tip**: Use coherence for stationary signals, wavelet coherence for non-stationary!
""",

    "arima_sarima": """
## ‚è±Ô∏è ARIMA/SARIMA Forecasting

### ARIMA(p, d, q)
- **p**: Autoregressive order (use PACF cutoff)
- **d**: Differencing order (0 if stationary)
- **q**: Moving average order (use ACF cutoff)

### SARIMA(p,d,q)(P,D,Q,m)
Adds seasonal components:
- **P, D, Q**: Seasonal AR, differencing, MA orders
- **m**: Seasonal period (12=monthly, 4=quarterly)

### Auto-ARIMA
Automatic parameter selection via grid search:
- Tests multiple (p,d,q) combinations
- Selects model with lowest AIC/BIC

### Forecasting
- Point forecasts with confidence intervals
- Residual diagnostics (should be white noise)
- Use holdout set for validation

üí° **Tip**: Always check stationarity (ADF test) before ARIMA modeling!
""",

    "multivariate_ts": """
## üìà Multivariate Time Series

### VAR (Vector Autoregression)
Models multiple time series jointly.
- Each variable depends on lagged values of all variables
- Includes Granger causality tests
- Good for forecasting interrelated variables

### VECM (Vector Error Correction)
Use when series are cointegrated (share long-run equilibrium).
- Johansen test identifies cointegration rank
- Models both short-run dynamics and long-run relationships
- Better than VAR for non-stationary but cointegrated series

### DTW (Dynamic Time Warping)
Measures similarity between time series that may be shifted/stretched.
- Handles different lengths and speeds
- Finds optimal alignment between series
- Use for pattern matching and clustering

### Granger Causality
Tests if past values of X help predict Y.
- Not true causation, but predictive causality
- Check at multiple lags

üí° **Tip**: Use VECM if Johansen test shows cointegration, VAR otherwise!
""",

    "biomass_segmentation": """
## üåø Biomass Segmentation (U-Net)

### Overview
Semantic segmentation for aquatic biomass using deep learning.
Classes: Background, Dreissena mussels, Cladophora algae, Other vegetation

### U-Net Architecture
- Encoder: Extracts features (downsampling)
- Decoder: Reconstructs spatial resolution (upsampling)
- Skip connections: Preserve fine details

### Training Tips
- Use augmentation for small datasets
- Dice loss works better than cross-entropy for imbalanced masks
- Start with pretrained VGG encoder for transfer learning

### Output Analysis
- Per-class coverage percentages
- Spatial distribution maps
- Biomass density estimates

üí° **Tip**: Transfer learning with VGG backbone works best for small datasets!
""",
}


def render_tutorial_sidebar():
    """Render the tutorial sidebar matching the tab/subtab structure"""
    with st.sidebar:
        st.markdown("# üìö Help & Documentation")

        st.session_state.show_tutorial = st.checkbox(
            "Show Documentation Panel",
            value=st.session_state.show_tutorial
        )

        if st.session_state.show_tutorial:
            st.markdown("---")
            
            # Toggle between short and comprehensive tutorials
            tutorial_mode = st.radio(
                "Documentation Mode",
                ["üìù Quick Reference", "üìñ Comprehensive Guide"],
                key="tutorial_mode",
                horizontal=True
            )
            use_comprehensive = tutorial_mode == "üìñ Comprehensive Guide"

            # Match the exact tab/subtab structure
            st.markdown("**Select a topic:**")

            tutorial_topics = {
                # Getting started
                "getting_started": "üöÄ Getting Started",
                # Data group
                "data_loading": "üìÅ Data ‚Ä∫ Data Loading",
                # Statistics group
                "statistical": "üìä Statistics ‚Ä∫ Descriptive Statistics",
                "tests": "üìä Statistics ‚Ä∫ Hypothesis Tests",
                "anova_extended": "üìä Statistics ‚Ä∫ ANOVA (Extended)",
                "probability_distributions": "üìä Statistics ‚Ä∫ Probability Distributions",
                "bayesian": "üìä Statistics ‚Ä∫ Bayesian Inference",
                "uncertainty": "üìä Statistics ‚Ä∫ Uncertainty Analysis",
                # Signal Processing group
                "signal_analysis": "üîä Signal Processing ‚Ä∫ FFT/Wavelet",
                "spectral_extended": "üîä Signal Processing ‚Ä∫ Extended Spectral",
                # Time Series group
                "timeseries": "‚è±Ô∏è Time Series ‚Ä∫ Analysis",
                "arima_sarima": "‚è±Ô∏è Time Series ‚Ä∫ ARIMA/SARIMA",
                "multivariate_ts": "‚è±Ô∏è Time Series ‚Ä∫ Multivariate (VAR/VECM/DTW)",
                "causality": "‚è±Ô∏è Time Series ‚Ä∫ Causality (Granger)",
                # Machine Learning group
                "machine_learning": "ü§ñ ML ‚Ä∫ Regression/Classification",
                "neural_networks": "üß† ML ‚Ä∫ Neural Networks",
                "image_recognition": "üñºÔ∏è Image Recognition ‚Ä∫ Streamlit Tab",
                "biomass_segmentation": "üåø ML ‚Ä∫ Biomass Segmentation",
                "pca": "ü§ñ ML ‚Ä∫ PCA (Principal Components)",
                "clustering": "ü§ñ ML ‚Ä∫ Clustering",
                "anomaly": "ü§ñ ML ‚Ä∫ Anomaly Detection",
                "dim_reduction": "ü§ñ ML ‚Ä∫ Dimensionality Reduction",
                "nonlinear": "ü§ñ ML ‚Ä∫ Non-Linear Analysis",
                # Visualization group
                "visualization": "üìà Visualization ‚Ä∫ Plots",
            }

            # Use Streamlit's `key` to bind the selectbox directly to
            # `st.session_state.current_tutorial`. Add an `on_change` callback
            # that records the most recent selection to force an immediate
            # session-state update (helps with certain frontend update races).
            def _load_comprehensive_module():
                """Load `comprehensive_tutorial` without triggering heavy package imports.
                First try normal import; if that fails, load the module directly from file.
                """
                try:
                    from data_toolkit import comprehensive_tutorial as ct
                    return ct
                except Exception:
                    # Fallback: load module directly from file to avoid package-level imports
                    import importlib.util
                    from pathlib import Path
                    file_path = Path(__file__).resolve().parents[1] / "comprehensive_tutorial.py"
                    if not file_path.exists():
                        raise
                    spec = importlib.util.spec_from_file_location("comprehensive_tutorial_local", str(file_path))
                    mod = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(mod)
                    return mod

            selected = st.selectbox(
                "Documentation Topic",
                options=list(tutorial_topics.keys()),
                format_func=lambda x: tutorial_topics[x],
                key="current_tutorial",
            )

            # Visible status line so users can see the currently selected topic
            # immediately after clicking (useful for diagnosing double-click issues).
            st.markdown(f"**Selected:** {tutorial_topics.get(st.session_state.current_tutorial, st.session_state.current_tutorial)}")

            # Show a compact source indicator immediately under the selectbox so
            # users do not have to scroll to the content area to see where the
            # text is coming from (comprehensive file vs short sidebar snippet).
            try:
                comp = _load_comprehensive_module()
                comp_topics = comp.get_all_topics()
                cur = st.session_state.current_tutorial
                
                # Mapping from sidebar keys to comprehensive tutorial keys
                SIDEBAR_TO_COMP = {
                    "pca": "pca_analysis",
                    "image_recognition": "image_recognition",
                    "anova_extended": "anova_extended",
                    "probability_distributions": "probability_distributions",
                    "spectral_extended": "spectral_extended",
                    "arima_sarima": "arima_sarima",
                    "multivariate_ts": "multivariate_ts",
                    "biomass_segmentation": "biomass_segmentation",
                }

                comp_topic = None
                # Prefer exact comprehensive key match
                if cur in comp_topics:
                    comp_topic = cur
                # Allow explicit mapping for the set above
                elif cur in SIDEBAR_TO_COMP and SIDEBAR_TO_COMP[cur] in comp_topics:
                    comp_topic = SIDEBAR_TO_COMP[cur]

                if use_comprehensive and comp_topic:
                    source_text = "üìñ Comprehensive Guide"
                elif comp_topic:
                    source_text = "üìù Quick Reference (üìñ available)"
                else:
                    source_text = "üìù Quick Reference"

            except Exception:
                source_text = "üìù Quick Reference (comprehensive load failed)"

            st.markdown(f"**{source_text}**")

            # Render the tutorial content based on mode selection
            try:
                if use_comprehensive and 'comp' in locals() and comp is not None and 'comp_topic' in locals() and comp_topic:
                    st.markdown(comp.get_tutorial(comp_topic))
                else:
                    st.markdown(TUTORIALS.get(selected, "Topic not found in quick reference."))
            except Exception as e:
                # Show the short tutorial and surface a debug note
                st.markdown(TUTORIALS.get(selected, "Topic not found."))
                st.error(f"[Debug] Comprehensive tutorial load failed: {e}")

        st.markdown("---")

        # Backend toggle
        st.markdown("### ‚ö° Performance")
        rust_available = is_rust_available()

        if rust_available:
            use_rust = st.checkbox(
                "ü¶Ä Rust Acceleration",
                value=st.session_state.use_rust,
                help="Enable Rust backend for 10-50x speedup"
            )
            st.session_state.use_rust = use_rust
            AccelerationSettings.set_use_rust(use_rust)

            if use_rust:
                st.success("‚ö° Using Rust (Fast)")
            else:
                st.info("üêç Using Python")
        else:
            st.warning("üêç Python only")
            st.caption("Run `maturin develop --release` in rust_extensions/ for speedup")


